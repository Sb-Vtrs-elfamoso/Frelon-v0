{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910e59a1",
   "metadata": {},
   "source": [
    "# Fine-tuning ST SSD MobileNet v1 0.25 on FreLon COCO\n",
    "\n",
    "This notebook prepares the STMicroelectronics SSD MobileNet v1 0.25 baseline for fine-tuning on the FreLon dataset. The original Windows-only TensorFlow Lite loading sequence has been replaced with logic that downloads the `.h5` weights alongside the YAML configuration shipped in the STM32 model zoo. Data loading has also been updated to materialize uniform tensors from the ragged `.npz` annotations so they can be fed into training pipelines more easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce0577",
   "metadata": {},
   "source": [
    "## Environment setup\n",
    "Import packages, configure reproducibility, and describe file-system constants used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d050f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import tf_keras\n",
    "import yaml\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"tf_keras version: {tf_keras.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e15df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure deterministic behaviour where possible\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and remote resources\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "DATA_ROOT = NOTEBOOK_DIR / \"hornet-bees-2\"\n",
    "MODEL_VARIANT = \"st_ssd_mobilenet_v1_025_256\"\n",
    "MODEL_BASE_URL = (\n",
    "    \"https://raw.githubusercontent.com/STMicroelectronics/stm32ai-modelzoo/main/\"\n",
    "    \"object_detection/st_ssd_mobilenet_v1/ST_pretrainedmodel_public_dataset/coco_2017_person\"\n",
    ")\n",
    "MODEL_CACHE = NOTEBOOK_DIR / \"models_cache\" / MODEL_VARIANT\n",
    "MODEL_CACHE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_FILES = {\n",
    "    \"weights\": f\"{MODEL_VARIANT}.h5\",\n",
    "    \"config\": f\"{MODEL_VARIANT}_config.yaml\",\n",
    "}\n",
    "\n",
    "\n",
    "def download_file(url: str, destination: Path) -> Path:\n",
    "    \"Download a file from a URL if it does not already exist.\"\n",
    "    if destination.exists():\n",
    "        return destination\n",
    "    response = requests.get(url, timeout=60)\n",
    "    response.raise_for_status()\n",
    "    destination.write_bytes(response.content)\n",
    "    return destination\n",
    "\n",
    "\n",
    "def ensure_model_assets() -> Dict[str, Path]:\n",
    "    \"Retrieve model weights and configuration from the STM32 model zoo.\"\n",
    "    assets: Dict[str, Path] = {}\n",
    "    for key, filename in MODEL_FILES.items():\n",
    "        url = f\"{MODEL_BASE_URL}/{MODEL_VARIANT}/{filename}\"\n",
    "        path = MODEL_CACHE / filename\n",
    "        assets[key] = download_file(url, path)\n",
    "    return assets\n",
    "\n",
    "\n",
    "def parse_input_shape(raw_shape: str) -> Tuple[int, int, int]:\n",
    "    \"Parse the (H, W, C) tuple stored as a string in the YAML config.\"\n",
    "    shape = ast.literal_eval(raw_shape)\n",
    "    if len(shape) != 3:\n",
    "        raise ValueError(f\"Unexpected input shape {shape}\")\n",
    "    return tuple(int(dim) for dim in shape)\n",
    "\n",
    "\n",
    "def ensure_materialized_npz(npz_path: Path) -> None:\n",
    "    \"Validate that Git LFS placeholders have been pulled before loading numpy archives.\"\n",
    "    if not npz_path.exists():\n",
    "        raise FileNotFoundError(f\"Expected dataset file missing: {npz_path}\")\n",
    "    head = npz_path.read_bytes()[:64]\n",
    "    if b\"version https://git-lfs.github.com\" in head:\n",
    "        raise RuntimeError(\n",
    "            f\"{npz_path} is a Git LFS pointer. Run `git lfs pull` or download the FreLon dataset \"\n",
    "            \"before executing this notebook.\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8e6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download assets and instantiate the baseline SSD Mobilenet model\n",
    "assets = ensure_model_assets()\n",
    "print(\"Model assets cached:\", assets)\n",
    "\n",
    "with assets[\"config\"].open(\"r\", encoding=\"utf-8\") as cfg_file:\n",
    "    model_config = yaml.safe_load(cfg_file)\n",
    "\n",
    "input_shape = parse_input_shape(model_config[\"training\"][\"model\"][\"input_shape\"])\n",
    "class_names = model_config.get(\"dataset\", {}).get(\"class_names\", [\"hornet\"])\n",
    "print(f\"Input shape: {input_shape}\")\n",
    "print(f\"Classes declared in config: {class_names}\")\n",
    "\n",
    "# Load the STMicro `.h5` model using the legacy-compatible `tf_keras` loader\n",
    "st_model = tf_keras.models.load_model(assets[\"weights\"], compile=False)\n",
    "print(\"Model outputs:\", st_model.output_names)\n",
    "print(\"Detection heads shapes:\", [output.shape for output in st_model.outputs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b704973",
   "metadata": {},
   "source": [
    "## Data pipeline helpers\n",
    "Functions below load the FreLon `.npz` exports, normalise the images, and convert ragged label structures into padded tensors. The padded representation captures bounding boxes, one-hot encoded classes, and a mask marking real boxes versus padding so downstream training code can create dense tensors matching the detection heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebcdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_split(split: str) -> Tuple[np.ndarray, List[Dict[str, np.ndarray]]]:\n",
    "    \"Load images and raw annotations for a given split.\"\n",
    "    npz_path = DATA_ROOT / f\"{split}_frelon.npz\"\n",
    "    ensure_materialized_npz(npz_path)\n",
    "    with np.load(npz_path, allow_pickle=True) as data:\n",
    "        images = data[\"X\"].astype(np.float32)\n",
    "        annotations = list(data[\"y\"])\n",
    "    return images, annotations\n",
    "\n",
    "\n",
    "def normalise_images(images: np.ndarray) -> np.ndarray:\n",
    "    return images / 255.0\n",
    "\n",
    "\n",
    "def pad_annotations(\n",
    "    annotations: List[Dict[str, np.ndarray]],\n",
    "    num_classes: int,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"Convert ragged detection targets into dense arrays.\"\n",
    "    max_boxes = max(len(entry[\"boxes\"]) for entry in annotations)\n",
    "    batch_size = len(annotations)\n",
    "    boxes = np.zeros((batch_size, max_boxes, 4), dtype=np.float32)\n",
    "    classes = np.zeros((batch_size, max_boxes, num_classes), dtype=np.float32)\n",
    "    mask = np.zeros((batch_size, max_boxes), dtype=np.float32)\n",
    "    for idx, entry in enumerate(annotations):\n",
    "        entry_boxes = np.asarray(entry[\"boxes\"], dtype=np.float32)\n",
    "        entry_class_ids = np.asarray(entry.get(\"class_ids\", entry.get(\"classes\")), dtype=np.int32)\n",
    "        count = len(entry_boxes)\n",
    "        if count == 0:\n",
    "            continue\n",
    "        boxes[idx, :count] = entry_boxes\n",
    "        classes[idx, np.arange(count), entry_class_ids] = 1.0\n",
    "        mask[idx, :count] = 1.0\n",
    "    return boxes, classes, mask\n",
    "\n",
    "\n",
    "def summarise_split(name: str, images: np.ndarray, annotations: List[Dict[str, np.ndarray]], num_classes: int) -> None:\n",
    "    boxes, classes, mask = pad_annotations(annotations, num_classes)\n",
    "    print(f\"{name} images: {images.shape}\")\n",
    "    print(f\"{name} boxes tensor shape: {boxes.shape}\")\n",
    "    print(f\"{name} classes tensor shape: {classes.shape}\")\n",
    "    print(f\"{name} mask tensor shape: {mask.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07063f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to materialise the FreLon dataset splits\n",
    "try:\n",
    "    train_images, train_ann = load_split(\"train\")\n",
    "    val_images, val_ann = load_split(\"val\")\n",
    "    test_images, test_ann = load_split(\"test\")\n",
    "    summarise_split(\"Train\", train_images, train_ann, len(class_names))\n",
    "    summarise_split(\"Validation\", val_images, val_ann, len(class_names))\n",
    "    summarise_split(\"Test\", test_images, test_ann, len(class_names))\n",
    "except Exception as error:\n",
    "    print(f\"Dataset unavailable: {error}\")\n",
    "    train_images = val_images = test_images = None\n",
    "    train_ann = val_ann = test_ann = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db47e0b",
   "metadata": {},
   "source": [
    "## Fine-tuning workflow (placeholder)\n",
    "The FreLon data can now be converted into dense tensors, but training requires anchor matching against the SSD detection heads. Implementing that matching logic is outside the scope of this automated refactor. The cell below illustrates where a fine-tuning loop would live once the dataset has been fully materialised and encoded for the SSD outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_images is None:\n",
    "    raise RuntimeError(\n",
    "        \"Training cannot start because the FreLon dataset archives were not available. \"\n",
    "        \"Download the `.npz` files (or run `git lfs pull`) and re-run the data-loading cell.\"\n",
    "    )\n",
    "\n",
    "# TODO: Implement SSD anchor matching and compile the model with appropriate losses.\n",
    "# Placeholder to indicate where fine-tuning would be triggered once targets are aligned with the 6,825 anchors.\n",
    "# st_model.compile(...)\n",
    "# history = st_model.fit(...)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
