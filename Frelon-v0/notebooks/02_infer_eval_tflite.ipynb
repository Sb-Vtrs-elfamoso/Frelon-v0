{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64183861",
   "metadata": {},
   "source": [
    "# 02 — Inférence & Évaluation (TFLite)\n",
    "\n",
    "- Charge des modèles **TFLite** déjà fournis (ex: SSD MobileNet v2, tiny YOLO nano, YOLOv8n quant.).\n",
    "- Fait l'inférence sur l'ensemble **test** (classe unique: *frelon*).\n",
    "- Calcule Precision/Recall/mAP (COCO-like) pour cette classe uniquement.\n",
    "- Visualise quelques prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a163fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images test: 45\n",
      "Annotations test: [{'id': 0, 'image_id': 0, 'category_id': 1, 'bbox': [113, 205, 36.5, 74.5], 'area': 2719.25, 'segmentation': [], 'iscrowd': 0}, {'id': 3, 'image_id': 3, 'category_id': 1, 'bbox': [369, 149, 45, 38.5], 'area': 1732.5, 'segmentation': [], 'iscrowd': 0}, {'id': 6, 'image_id': 6, 'category_id': 1, 'bbox': [159, 179, 41.5, 69.5], 'area': 2884.25, 'segmentation': [], 'iscrowd': 0}, {'id': 7, 'image_id': 6, 'category_id': 1, 'bbox': [228, 372, 45.5, 43.5], 'area': 1979.25, 'segmentation': [], 'iscrowd': 0}, {'id': 8, 'image_id': 7, 'category_id': 1, 'bbox': [222, 128, 31, 48.5], 'area': 1503.5, 'segmentation': [], 'iscrowd': 0}, {'id': 9, 'image_id': 7, 'category_id': 1, 'bbox': [157, 208, 39, 48.5], 'area': 1891.5, 'segmentation': [], 'iscrowd': 0}, {'id': 10, 'image_id': 8, 'category_id': 1, 'bbox': [114, 229, 38, 58], 'area': 2204, 'segmentation': [], 'iscrowd': 0}, {'id': 12, 'image_id': 10, 'category_id': 1, 'bbox': [43, 262, 49, 54.5], 'area': 2670.5, 'segmentation': [], 'iscrowd': 0}, {'id': 13, 'image_id': 11, 'category_id': 1, 'bbox': [290, 198, 47.5, 52.5], 'area': 2493.75, 'segmentation': [], 'iscrowd': 0}, {'id': 14, 'image_id': 12, 'category_id': 1, 'bbox': [226, 258, 36.5, 71], 'area': 2591.5, 'segmentation': [], 'iscrowd': 0}, {'id': 16, 'image_id': 14, 'category_id': 1, 'bbox': [241, 112, 23.5, 55], 'area': 1292.5, 'segmentation': [], 'iscrowd': 0}, {'id': 18, 'image_id': 16, 'category_id': 1, 'bbox': [306, 21, 27, 58], 'area': 1566, 'segmentation': [], 'iscrowd': 0}, {'id': 19, 'image_id': 17, 'category_id': 1, 'bbox': [126, 265, 49, 81.5], 'area': 3993.5, 'segmentation': [], 'iscrowd': 0}, {'id': 20, 'image_id': 18, 'category_id': 1, 'bbox': [5, 220, 45, 73.5], 'area': 3307.5, 'segmentation': [], 'iscrowd': 0}, {'id': 21, 'image_id': 19, 'category_id': 1, 'bbox': [1, 273, 30, 49], 'area': 1470, 'segmentation': [], 'iscrowd': 0}, {'id': 22, 'image_id': 20, 'category_id': 1, 'bbox': [244, 241, 36, 81.5], 'area': 2934, 'segmentation': [], 'iscrowd': 0}, {'id': 23, 'image_id': 21, 'category_id': 1, 'bbox': [156, 185, 36, 61.5], 'area': 2214, 'segmentation': [], 'iscrowd': 0}, {'id': 24, 'image_id': 21, 'category_id': 1, 'bbox': [214, 327, 41.5, 61], 'area': 2531.5, 'segmentation': [], 'iscrowd': 0}, {'id': 26, 'image_id': 23, 'category_id': 1, 'bbox': [310, 357, 50.5, 56], 'area': 2828, 'segmentation': [], 'iscrowd': 0}, {'id': 27, 'image_id': 24, 'category_id': 1, 'bbox': [139, 215, 49, 7.5], 'area': 367.5, 'segmentation': [], 'iscrowd': 0}, {'id': 28, 'image_id': 24, 'category_id': 1, 'bbox': [208, 365, 36, 51], 'area': 1836, 'segmentation': [], 'iscrowd': 0}, {'id': 29, 'image_id': 25, 'category_id': 1, 'bbox': [340, 92, 42, 83], 'area': 3486, 'segmentation': [], 'iscrowd': 0}, {'id': 30, 'image_id': 26, 'category_id': 1, 'bbox': [335, 14, 33, 58.5], 'area': 1930.5, 'segmentation': [], 'iscrowd': 0}, {'id': 31, 'image_id': 27, 'category_id': 1, 'bbox': [82, 182, 46, 57], 'area': 2622, 'segmentation': [], 'iscrowd': 0}, {'id': 32, 'image_id': 28, 'category_id': 1, 'bbox': [147, 146, 19, 34.5], 'area': 655.5, 'segmentation': [], 'iscrowd': 0}, {'id': 33, 'image_id': 28, 'category_id': 1, 'bbox': [264, 281, 33, 78], 'area': 2574, 'segmentation': [], 'iscrowd': 0}, {'id': 34, 'image_id': 29, 'category_id': 1, 'bbox': [302, 41, 30.5, 23], 'area': 701.5, 'segmentation': [], 'iscrowd': 0}, {'id': 35, 'image_id': 30, 'category_id': 1, 'bbox': [191, 234, 42, 69.5], 'area': 2919, 'segmentation': [], 'iscrowd': 0}, {'id': 36, 'image_id': 31, 'category_id': 1, 'bbox': [231, 344, 43.5, 71.5], 'area': 3110.25, 'segmentation': [], 'iscrowd': 0}, {'id': 39, 'image_id': 33, 'category_id': 1, 'bbox': [61, 310, 45.5, 91.5], 'area': 4163.25, 'segmentation': [], 'iscrowd': 0}, {'id': 40, 'image_id': 34, 'category_id': 1, 'bbox': [156, 154, 22.5, 7.5], 'area': 168.75, 'segmentation': [], 'iscrowd': 0}, {'id': 41, 'image_id': 35, 'category_id': 1, 'bbox': [147, 207, 36, 71], 'area': 2556, 'segmentation': [], 'iscrowd': 0}, {'id': 42, 'image_id': 36, 'category_id': 1, 'bbox': [115, 272, 42, 27], 'area': 1134, 'segmentation': [], 'iscrowd': 0}, {'id': 44, 'image_id': 37, 'category_id': 1, 'bbox': [243, 112, 19.5, 55], 'area': 1072.5, 'segmentation': [], 'iscrowd': 0}, {'id': 45, 'image_id': 38, 'category_id': 1, 'bbox': [300, 156, 36, 69.5], 'area': 2502, 'segmentation': [], 'iscrowd': 0}, {'id': 46, 'image_id': 39, 'category_id': 1, 'bbox': [82, 178, 36, 84.5], 'area': 3042, 'segmentation': [], 'iscrowd': 0}, {'id': 47, 'image_id': 40, 'category_id': 1, 'bbox': [151, 249, 39.5, 27], 'area': 1066.5, 'segmentation': [], 'iscrowd': 0}, {'id': 48, 'image_id': 41, 'category_id': 1, 'bbox': [246, 70, 33, 56], 'area': 1848, 'segmentation': [], 'iscrowd': 0}, {'id': 49, 'image_id': 41, 'category_id': 1, 'bbox': [54, 195, 34, 7.5], 'area': 255, 'segmentation': [], 'iscrowd': 0}, {'id': 50, 'image_id': 42, 'category_id': 1, 'bbox': [170, 211, 45.5, 54.5], 'area': 2479.75, 'segmentation': [], 'iscrowd': 0}, {'id': 52, 'image_id': 44, 'category_id': 1, 'bbox': [79, 197, 36, 31], 'area': 1116, 'segmentation': [], 'iscrowd': 0}, {'id': 53, 'image_id': 45, 'category_id': 1, 'bbox': [344, 216, 34.5, 78], 'area': 2691, 'segmentation': [], 'iscrowd': 0}, {'id': 54, 'image_id': 46, 'category_id': 1, 'bbox': [367, 173, 33, 27], 'area': 891, 'segmentation': [], 'iscrowd': 0}, {'id': 55, 'image_id': 47, 'category_id': 1, 'bbox': [124, 203, 28.5, 75], 'area': 2137.5, 'segmentation': [], 'iscrowd': 0}, {'id': 56, 'image_id': 48, 'category_id': 1, 'bbox': [94, 183, 48.5, 44.5], 'area': 2158.25, 'segmentation': [], 'iscrowd': 0}, {'id': 58, 'image_id': 49, 'category_id': 1, 'bbox': [249, 144, 50, 47.5], 'area': 2375, 'segmentation': [], 'iscrowd': 0}, {'id': 59, 'image_id': 50, 'category_id': 1, 'bbox': [52, 35, 30.5, 34.5], 'area': 1052.25, 'segmentation': [], 'iscrowd': 0}, {'id': 60, 'image_id': 51, 'category_id': 1, 'bbox': [174, 174, 39, 76.5], 'area': 2983.5, 'segmentation': [], 'iscrowd': 0}, {'id': 61, 'image_id': 51, 'category_id': 1, 'bbox': [265, 369, 24.5, 47], 'area': 1151.5, 'segmentation': [], 'iscrowd': 0}, {'id': 62, 'image_id': 52, 'category_id': 1, 'bbox': [179, 172, 47, 59], 'area': 2773, 'segmentation': [], 'iscrowd': 0}, {'id': 63, 'image_id': 52, 'category_id': 1, 'bbox': [299, 368, 32, 48], 'area': 1536, 'segmentation': [], 'iscrowd': 0}, {'id': 64, 'image_id': 53, 'category_id': 1, 'bbox': [267, 20, 40.5, 59.5], 'area': 2409.75, 'segmentation': [], 'iscrowd': 0}, {'id': 65, 'image_id': 54, 'category_id': 1, 'bbox': [292, 292, 41.5, 76.5], 'area': 3174.75, 'segmentation': [], 'iscrowd': 0}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, json, cv2, os, itertools, math\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = Path(\"hornet-bees-2\")  # dossier créé par Roboflow download\n",
    "assert DATA_DIR.exists(), f\"{DATA_DIR} introuvable. Exécutez d'abord 01_data.ipynb.\"\n",
    "\n",
    "TEST_DIR = DATA_DIR / \"test\"\n",
    "TEST_ANN = TEST_DIR / \"_annotations_frelon_single.coco.json\"\n",
    "with open(TEST_ANN, \"r\", encoding=\"utf-8\") as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "id_to_img = {im[\"id\"]: im for im in coco[\"images\"]}\n",
    "gts_by_img = {}\n",
    "for a in coco[\"annotations\"]:\n",
    "    gts_by_img.setdefault(a[\"image_id\"], []).append(a)\n",
    "\n",
    "print(\"Images test:\", len(coco[\"images\"]))\n",
    "print(\"Annotations test:\", coco[\"annotations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc589701",
   "metadata": {},
   "source": [
    "## Utilitaires TFLite (loader générique + post-process naïf NMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659201bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tflite_interpreter(tflite_path):\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    return interpreter, input_details, output_details\n",
    "\n",
    "def preprocess_image(path, input_size):\n",
    "    img = cv2.imread(str(path))\n",
    "    h0, w0 = img.shape[:2]\n",
    "    img_resized = cv2.resize(img, input_size, interpolation=cv2.INTER_LINEAR)\n",
    "    # Normalisation simple 0-255 -> 0-1 (change if model expects something else)\n",
    "    img_float = img_resized.astype(np.float32) / 255.0\n",
    "    img_batched = np.expand_dims(img_float, 0)\n",
    "    return img, (h0, w0), img_batched\n",
    "\n",
    "def nms(boxes, scores, iou_thr=0.5, top_k=100):\n",
    "    # boxes: [N,4] in xyxy\n",
    "    idxs = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "    while idxs.size > 0 and len(keep) < top_k:\n",
    "        i = idxs[0]\n",
    "        keep.append(i)\n",
    "        if idxs.size == 1: break\n",
    "        ious = []\n",
    "        for j in idxs[1:]:\n",
    "            xx1 = max(boxes[i,0], boxes[j,0])\n",
    "            yy1 = max(boxes[i,1], boxes[j,1])\n",
    "            xx2 = min(boxes[i,2], boxes[j,2])\n",
    "            yy2 = min(boxes[i,3], boxes[j,3])\n",
    "            inter = max(0, xx2-xx1) * max(0, yy2-yy1)\n",
    "            area_i = (boxes[i,2]-boxes[i,0])*(boxes[i,3]-boxes[i,1])\n",
    "            area_j = (boxes[j,2]-boxes[j,0])*(boxes[j,3]-boxes[j,1])\n",
    "            union = area_i + area_j - inter + 1e-6\n",
    "            ious.append(inter/union)\n",
    "        idxs = idxs[1:][np.array(ious) <= iou_thr]\n",
    "    return np.array(keep, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e960ef22",
   "metadata": {},
   "source": [
    "## Adaptateurs simples par modèle (à ajuster selon tes exports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51dd9f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def infer_ssd_mobilenet(interpreter, input_details, output_details, img_batched, orig_size):\n",
    "    # Hypothèse TFOD standard: outputs: boxes [1,N,4] in y1,x1,y2,x2 normalized, scores [1,N], classes [1,N]\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], img_batched.astype(np.float32))\n",
    "    interpreter.invoke()\n",
    "    boxes = interpreter.get_tensor(output_details[0][\"index\"])[0]  # [N,4]\n",
    "    classes = interpreter.get_tensor(output_details[1][\"index\"])[0] # [N]\n",
    "    scores = interpreter.get_tensor(output_details[2][\"index\"])[0]  # [N]\n",
    "    h, w = orig_size\n",
    "    # convert to xyxy pixel\n",
    "    xyxy = np.stack([boxes[:,1]*w, boxes[:,0]*h, boxes[:,3]*w, boxes[:,2]*h], axis=1)\n",
    "    return xyxy, scores, classes\n",
    "\n",
    "def infer_yolo_like(interpreter, input_details, output_details, img_batched, orig_size):\n",
    "    # Très dépendant du modèle; ici on suppose une sortie unique [1,Num,6] (x,y,w,h,conf,cls)\n",
    "    interpreter.set_tensor(input_details[0][\"index\"], img_batched.astype(np.float32))\n",
    "    interpreter.invoke()\n",
    "    out = []\n",
    "    for od in output_details:\n",
    "        out.append(interpreter.get_tensor(od[\"index\"]))\n",
    "    # concat sur l'axe des anchors si besoin\n",
    "    pred = np.concatenate([o.reshape(1,-1,o.shape[-1]) for o in out], axis=1)[0]  # [N,6] or [N,??]\n",
    "    # Try to parse x,y,w,h,conf,cls\n",
    "    if pred.shape[-1] < 6:\n",
    "        raise ValueError(\"Sortie YOLO inattendue; ajuste 'infer_yolo_like'.\")\n",
    "    x, y, w, h, conf, cls = [pred[:,i] for i in range(6)]\n",
    "    # Convert cxcywh -> xyxy\n",
    "    cx, cy = x, y\n",
    "    x1 = (cx - w/2.0) * orig_size[1]\n",
    "    y1 = (cy - h/2.0) * orig_size[0]\n",
    "    x2 = (cx + w/2.0) * orig_size[1]\n",
    "    y2 = (cy + h/2.0) * orig_size[0]\n",
    "    xyxy = np.stack([x1,y1,x2,y2], axis=1)\n",
    "    scores = conf\n",
    "    classes = cls\n",
    "    return xyxy, scores, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9eddb",
   "metadata": {},
   "source": [
    "## Évaluation (AP/AR simplifiés pour 1 classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fccc15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_xyxy(a, b):\n",
    "    xx1 = max(a[0], b[0]); yy1 = max(a[1], b[1])\n",
    "    xx2 = min(a[2], b[2]); yy2 = min(a[3], b[3])\n",
    "    inter = max(0, xx2-xx1) * max(0, yy2-yy1)\n",
    "    area_a = (a[2]-a[0])*(a[3]-a[1])\n",
    "    area_b = (b[2]-b[0])*(b[3]-b[1])\n",
    "    return inter / (area_a + area_b - inter + 1e-6)\n",
    "\n",
    "def eval_detections(preds_by_img, gts_by_img, iou_thr=0.5):\n",
    "    # preds_by_img: dict[img_id] -> list of (xyxy, score)\n",
    "    # gts_by_img: dict[img_id] -> list of ann with 'bbox' [x,y,w,h]\n",
    "    all_scores, all_matches = [], []\n",
    "    n_gt = 0\n",
    "    for img_id, gts in gts_by_img.items():\n",
    "        n_gt += len(gts)\n",
    "        gts_used = [False]*len(gts)\n",
    "        preds = preds_by_img.get(img_id, [])\n",
    "        preds = sorted(preds, key=lambda x: x[1], reverse=True)\n",
    "        for (box, s) in preds:\n",
    "            match = 0\n",
    "            for i,gt in enumerate(gts):\n",
    "                gx,gy,gw,gh = gt[\"bbox\"]\n",
    "                gxyxy = [gx,gy,gx+gw,gy+gh]\n",
    "                if not gts_used[i] and iou_xyxy(box, gxyxy) >= iou_thr:\n",
    "                    gts_used[i] = True\n",
    "                    match = 1\n",
    "                    break\n",
    "            all_scores.append(s)\n",
    "            all_matches.append(match)\n",
    "    # Compute Precision-Recall curve + AP (11-point)\n",
    "    order = np.argsort(-np.array(all_scores))\n",
    "    tp = np.cumsum(np.array(all_matches)[order])\n",
    "    fp = np.cumsum(1 - np.array(all_matches)[order])\n",
    "    recalls = tp / max(n_gt, 1)\n",
    "    precisions = tp / np.maximum(tp+fp, 1e-9)\n",
    "    # 11-point interpolated AP\n",
    "    ap = 0.0\n",
    "    for r in np.linspace(0,1,11):\n",
    "        p = 0.0\n",
    "        mask = recalls >= r\n",
    "        if mask.any():\n",
    "            p = np.max(precisions[mask])\n",
    "        ap += p/11.0\n",
    "    return {\"AP@0.5\": float(ap), \"nGT\": int(n_gt)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba79ad6",
   "metadata": {},
   "source": [
    "## Exécuter l'inférence pour chaque modèle disponible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb52052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèles trouvés: {'ssd_mobilenet': WindowsPath('../src/models/ssd_mobilenet_v2_fpnlite_035_416_int8.tflite'), 'tiny_yolo_nano': WindowsPath('../src/models/st_yolo_x_nano_416_0.33_0.25_int8.tflite'), 'yolov8n_quant': WindowsPath('../src/models/yolov8n_416_quant_pc_uf_od_coco-person.tflite')}\n",
      "==> ssd_mobilenet: ..\\src\\models\\ssd_mobilenet_v2_fpnlite_035_416_int8.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 149.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssd_mobilenet {'AP@0.5': 0.0, 'nGT': 53}\n",
      "==> tiny_yolo_nano: ..\\src\\models\\st_yolo_x_nano_416_0.33_0.25_int8.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 219.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiny_yolo_nano {'AP@0.5': 0.0, 'nGT': 53}\n",
      "==> yolov8n_quant: ..\\src\\models\\yolov8n_416_quant_pc_uf_od_coco-person.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:00<00:00, 187.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yolov8n_quant {'AP@0.5': 0.0, 'nGT': 53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ssd_mobilenet': {'AP@0.5': 0.0, 'nGT': 53},\n",
       " 'tiny_yolo_nano': {'AP@0.5': 0.0, 'nGT': 53},\n",
       " 'yolov8n_quant': {'AP@0.5': 0.0, 'nGT': 53}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODELS_DIR = Path(\"../src/models\")\n",
    "candidates = {\n",
    "    \"ssd_mobilenet\": MODELS_DIR / \"ssd_mobilenet_v2_fpnlite_035_416_int8.tflite\",\n",
    "    \"tiny_yolo_nano\": MODELS_DIR / \"st_yolo_x_nano_416_0.33_0.25_int8.tflite\",\n",
    "    \"yolov8n_quant\": MODELS_DIR / \"yolov8n_416_quant_pc_uf_od_coco-person.tflite\",\n",
    "}\n",
    "available = {k:v for k,v in candidates.items() if v.exists()}\n",
    "print(\"Modèles trouvés:\", available)\n",
    "\n",
    "results = {}\n",
    "viz_dir = Path(\"predictions_viz\"); viz_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Map COCO image_id -> file path\n",
    "img_path_by_id = {im[\"id\"]: TEST_DIR / im[\"file_name\"] for im in coco[\"images\"]}\n",
    "\n",
    "for name, path in available.items():\n",
    "    print(f\"==> {name}:\", path)\n",
    "    interpreter, inp, out = load_tflite_interpreter(path)\n",
    "    # derive input size\n",
    "    _, in_h, in_w, _ = inp[0][\"shape\"]\n",
    "    preds_by_img = {}\n",
    "    for img_id, im_meta in tqdm(id_to_img.items(), total=len(id_to_img)):\n",
    "        if (TEST_DIR / im_meta[\"file_name\"]).exists():\n",
    "            img_bgr, (h0,w0), img_batched = preprocess_image(TEST_DIR / im_meta[\"file_name\"], (in_w, in_h))\n",
    "            try:\n",
    "                if \"ssd_mobilenet\" in name:\n",
    "                    xyxy, scores, classes = infer_ssd_mobilenet(interpreter, inp, out, img_batched, (h0,w0))\n",
    "                else:\n",
    "                    xyxy, scores, classes = infer_yolo_like(interpreter, inp, out, img_batched, (h0,w0))\n",
    "            except Exception as e:\n",
    "                # skip image if incompatible\n",
    "                continue\n",
    "            # garder uniquement frelon (cls 0/1 varie selon modèle; ici on prend tout et NMS)\n",
    "            keep = scores > 0.25\n",
    "            xyxy = xyxy[keep]; scores_f = scores[keep]\n",
    "            if xyxy.size:\n",
    "                k = nms(xyxy, scores_f, iou_thr=0.5, top_k=50)\n",
    "                preds_by_img[img_id] = [(xyxy[i], float(scores_f[i])) for i in k]\n",
    "            else:\n",
    "                preds_by_img[img_id] = []\n",
    "    metrics = eval_detections(preds_by_img, gts_by_img, iou_thr=0.5)\n",
    "    results[name] = metrics\n",
    "    print(name, metrics)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d95472",
   "metadata": {},
   "source": [
    "## Visualisation qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "abc3e515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: ssd_mobilenet\n"
     ]
    }
   ],
   "source": [
    "def draw_boxes(img, boxes, color=(0,255,0)):\n",
    "    for b in boxes:\n",
    "        x1,y1,x2,y2 = map(int, b)\n",
    "        cv2.rectangle(img, (x1,y1), (x2,y2), color, 2)\n",
    "    return img\n",
    "\n",
    "# Afficher quelques images avec prédictions du meilleur modèle (AP max)\n",
    "best = max(results.items(), key=lambda kv: kv[1][\"AP@0.5\"])[0] if results else None\n",
    "print(\"Best model:\", best)\n",
    "if best:\n",
    "    # Recharge interpréteur pour dessiner 8 images\n",
    "    interpreter, inp, out = load_tflite_interpreter(candidates[best])\n",
    "    _, in_h, in_w, _ = inp[0][\"shape\"]\n",
    "    shown = 0\n",
    "    for img_id, meta in id_to_img.items():\n",
    "        p = TEST_DIR / meta[\"file_name\"]\n",
    "        if not p.exists(): continue\n",
    "        img_bgr, (h0,w0), img_batched = preprocess_image(p, (in_w, in_h))\n",
    "        try:\n",
    "            if \"ssd_mobilenet\" in best:\n",
    "                xyxy, scores, classes = infer_ssd_mobilenet(interpreter, inp, out, img_batched, (h0,w0))\n",
    "            else:\n",
    "                xyxy, scores, classes = infer_yolo_like(interpreter, inp, out, img_batched, (h0,w0))\n",
    "        except:\n",
    "            continue\n",
    "        keep = scores > 0.25\n",
    "        xyxy = xyxy[keep]\n",
    "        if xyxy.size:\n",
    "            img_draw = draw_boxes(img_bgr.copy(), xyxy)\n",
    "            img_rgb = cv2.cvtColor(img_draw, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(6,6))\n",
    "            plt.imshow(img_rgb); plt.axis(\"off\"); plt.title(f\"{best}\")\n",
    "            plt.show()\n",
    "            shown += 1\n",
    "        if shown >= 8: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b9064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
