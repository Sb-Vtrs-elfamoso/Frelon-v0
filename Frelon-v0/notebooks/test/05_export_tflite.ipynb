{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aa3f5c6",
   "metadata": {},
   "source": [
    "# 05 — Export TFLite (optionnel)\n",
    "\n",
    "Exporter le modèle entraîné en TFLite (float32 ou quantifié int8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "TF_MODELS_DIR = \"models\"\n",
    "PIPELINE_CONFIG_PATH = \"training/ssd_mobilenet_v3_small_fpnlite_320x320_coco17_tpu-8/pipeline.config\"\n",
    "MODEL_DIR = \"checkpoints/ssd_mobilenet_v3_small_frelon\"\n",
    "EXPORT_DIR = \"exported_models/ssd_mobilenet_v3_small_frelon\"\n",
    "\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\"\"\n",
    "Exporter en SavedModel:\n",
    "\n",
    "python {TF_MODELS_DIR}/research/object_detection/exporter_main_v2.py \\\n",
    "  --input_type=image_tensor \\\n",
    "  --pipeline_config_path={PIPELINE_CONFIG_PATH} \\\n",
    "  --trained_checkpoint_dir={MODEL_DIR} \\\n",
    "  --output_directory={EXPORT_DIR}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Puis conversion en TFLite:\n",
    "\n",
    "python - <<'PY'\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "saved_model_dir = r\"{EXPORT_DIR}/saved_model\"\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()\n",
    "pathlib.Path(\"{EXPORT_DIR}/model.tflite\").write_bytes(tflite_model)\n",
    "print(\"TFLite exporté vers {EXPORT_DIR}/model.tflite\")\n",
    "PY\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Quantification int8 (post-training) — exemple minimal:\n",
    "\n",
    "python - <<'PY'\n",
    "import tensorflow as tf, pathlib\n",
    "saved_model_dir = r\"{EXPORT_DIR}/saved_model\"\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Optionnel: jeu de calibration (recommandé)\n",
    "# converter.representative_dataset = ...\n",
    "# Pour garantir int8 pur (Edge TPU, microctrl.), définir les types:\n",
    "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# converter.inference_input_type = tf.int8\n",
    "# converter.inference_output_type = tf.int8\n",
    "quant_tflite = converter.convert()\n",
    "pathlib.Path(\"{EXPORT_DIR}/model_int8.tflite\").write_bytes(quant_tflite)\n",
    "print(\"TFLite int8 exporté vers {EXPORT_DIR}/model_int8.tflite\")\n",
    "PY\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
